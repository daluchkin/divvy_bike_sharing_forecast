{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5211eb81-4e64-472e-92d0-548e2343d59d",
   "metadata": {},
   "source": [
    "[__<< Feature Engineering__](./04_XXX_feature_engineering.ipynb) | [__Home__](../README.md)\n",
    "\n",
    "\n",
    "\n",
    "# [PROJECT NAME]\n",
    "## Modeling\n",
    "\n",
    "__Dataset:__ [DATASET](URL) \\\n",
    "__Author:__ [AUTHOR NAME](URL) \\\n",
    "__Version:__ 1.N.0\\\n",
    "__Date:__ [YYYY-MM-DD]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc80bf-4f07-4c1d-92e0-ad645880cc65",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Notebooks <a class=\"anchor\" name='notebooks'></a>\n",
    "\n",
    "+ [Initial Data Exploration](./01_XXX_data_exploration.ipynb)\n",
    "+ [Data Cleaning](./02_XXX_data_cleaning.ipynb)\n",
    "+ [Exploratory Data Analysis](./03_XXX_exploratory_data_analysis.ipynb)\n",
    "+ [Feature Engineering](./04_XXX_feature_engineering.ipynb)\n",
    "+ __[Modeling & Validation](./05_XXX_modeling.ipynb)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86b950e-799c-4bfd-8bdc-c5c98cb3d509",
   "metadata": {},
   "source": [
    "### Import Libraries <a name='#import-libraries'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4925e3cc-7b0d-4274-81d4-2600f68c2bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels as sm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('../02_scripts/')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13cebe8-a0dc-4bf2-a8ea-db8789ce17f3",
   "metadata": {},
   "source": [
    "### Notebook Setup <a name='#notebook-setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f2cce2-fa38-4f24-ba48-fa8cd2a5e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas settings\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_colwidth = 60\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "\n",
    "# Visualization settings\n",
    "from matplotlib import rcParams\n",
    "plt.style.use('fivethirtyeight')\n",
    "rcParams['figure.figsize'] = (16, 5)   \n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['font.size'] = 12\n",
    "rcParams['savefig.dpi'] = 300\n",
    "plt.rc('xtick', labelsize=11)\n",
    "plt.rc('ytick', labelsize=11)\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "133567a2-d2c2-4b83-9682-1e08e4dc7004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "\n",
    "@register_cell_magic\n",
    "def markdown(line, cell):\n",
    "    return Markdown(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdd024c-eaab-41d1-bd5a-6f8205cff1c9",
   "metadata": {},
   "source": [
    "### ToDo's <a name='todos'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef01d15f-8f37-41c7-a183-2dfd70794a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsys.path.append('../02_scripts/')\\nfrom todo_list import extract_todo_patterns\\n\\nprint(f'{'-'*5} TASKS FROM PREVIOUS PHASE {'-'*5}')\\nfor todo in extract_todo_patterns('./02_XXX_data_cleaning.ipynb'):\\n    print(f'TODO: {todo}')\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all tasks from the previous phase\n",
    "\n",
    "\"\"\"\n",
    "sys.path.append('../02_scripts/')\n",
    "from todo_list import extract_todo_patterns\n",
    "\n",
    "print(f'{'-'*5} TASKS FROM PREVIOUS PHASE {'-'*5}')\n",
    "for todo in extract_todo_patterns('./02_XXX_data_cleaning.ipynb'):\n",
    "    print(f'TODO: {todo}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a205ce82-2cff-4707-b06a-32573a6ea569",
   "metadata": {},
   "source": [
    "### Loading Data <a name='#loading-data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de1542a-93ba-410c-8f76-c1e553c66abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store -r myvar # to load dataset in the next phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6beab09d-791e-4f51-9f0e-0250fe8275ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "# filename = '../00_data/02_processed/used_cars_data_processed_final.pkl'\n",
    "\n",
    "# with open(filename, 'rb') as file:\n",
    "#     data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92d241d-f83b-4ae8-a809-e4a4299766f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f051eb1-2b52-47b2-bb89-3f37fe7cd590",
   "metadata": {},
   "source": [
    "### Building Model <a name='#building-model'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca4775-03b0-4a0c-a3e2-8f1034325e63",
   "metadata": {},
   "source": [
    "1. Supervised Learning\n",
    "Supervised learning algorithms are trained using labeled data, where the input-output pairs are known.\n",
    "\n",
    "    - Classification\n",
    "        - **Logistic Regression**\n",
    "        - **Support Vector Machines (SVM)**\n",
    "        - **k-Nearest Neighbors (k-NN)**\n",
    "        - **Decision Trees**\n",
    "        - **Random Forest**\n",
    "        - **Gradient Boosting Machines (e.g., XGBoost, LightGBM)**\n",
    "        - **Neural Networks (Multi-Layer Perceptron, Convolutional Neural Networks)**\n",
    "        - **Naive Bayes**\n",
    "\n",
    "    - Regression\n",
    "        - **Linear Regression**\n",
    "        - **Ridge Regression**\n",
    "        - **Lasso Regression**\n",
    "        - **Support Vector Regression (SVR)**\n",
    "        - **Decision Trees for Regression**\n",
    "        - **Random Forest for Regression**\n",
    "        - **Gradient Boosting for Regression**\n",
    "        - **Neural Networks for Regression**\n",
    "\n",
    "2. Unsupervised Learning\n",
    "Unsupervised learning algorithms are used on data without labeled responses, finding hidden patterns or intrinsic structures.\n",
    "\n",
    "    - Clustering\n",
    "        - **k-Means Clustering**\n",
    "        - **Hierarchical Clustering**\n",
    "        - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**\n",
    "        - **Gaussian Mixture Models (GMM)**\n",
    "\n",
    "    - Dimensionality Reduction\n",
    "        - **Principal Component Analysis (PCA)**\n",
    "        - **t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n",
    "        - **Linear Discriminant Analysis (LDA)**\n",
    "        - **Autoencoders**\n",
    "\n",
    "    - Association Rule Learning\n",
    "        - **Apriori Algorithm**\n",
    "        - **Eclat Algorithm**\n",
    "\n",
    "3. Semi-Supervised Learning\n",
    "Semi-supervised learning algorithms use both labeled and unlabeled data for training, typically a small amount of labeled data and a large amount of unlabeled data.\n",
    "\n",
    "    - **Self-Training**\n",
    "    - **Co-Training**\n",
    "    - **Semi-Supervised Support Vector Machines (S3VM)**\n",
    "    - **Generative Adversarial Networks (GANs) for Semi-Supervised Learning**\n",
    "\n",
    "4. Reinforcement Learning\n",
    "Reinforcement learning algorithms learn by interacting with an environment, receiving rewards or penalties.\n",
    "\n",
    "    - **Q-Learning**\n",
    "    - **Deep Q-Networks (DQN)**\n",
    "    - **Policy Gradient Methods**\n",
    "    - **Actor-Critic Methods**\n",
    "    - **Monte Carlo Methods**\n",
    "\n",
    "5. Ensemble Learning\n",
    "Ensemble learning methods combine multiple learning algorithms to improve performance.\n",
    "\n",
    "    - **Bagging (Bootstrap Aggregating)**\n",
    "    - **Boosting**\n",
    "    - **Stacking**\n",
    "    - **Voting**\n",
    "\n",
    "6. Neural Networks and Deep Learning\n",
    "A subset of machine learning focusing on neural networks with many layers.\n",
    "\n",
    "    - **Convolutional Neural Networks (CNNs)**\n",
    "    - **Recurrent Neural Networks (RNNs)**\n",
    "    - **Long Short-Term Memory Networks (LSTMs)**\n",
    "    - **Generative Adversarial Networks (GANs)**\n",
    "    - **Transformer Networks**\n",
    "\n",
    "7. Anomaly Detection\n",
    "Algorithms designed to identify rare items, events, or observations.\n",
    "\n",
    "    - **Isolation Forest**\n",
    "    - **One-Class SVM**\n",
    "    - **Autoencoders for Anomaly Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80808738-67d2-4286-be52-64173667c748",
   "metadata": {},
   "source": [
    "#### Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ce3234-95cd-43ae-8736-644edf0e6979",
   "metadata": {},
   "source": [
    "1. Cross-Validation\n",
    "\n",
    "    - K-Fold Cross-Validation\n",
    "        - **Definition**: The dataset is divided into K equal-sized folds. The model is trained on K-1 folds and validated on the remaining fold. This process is repeated K times, with each fold serving as the validation set once.\n",
    "        - **Purpose**: Provides a comprehensive evaluation of the model by using different subsets of the data for training and validation.\n",
    "        - **Advantages**: Reduces bias associated with random sampling, more stable and reliable estimates of model performance.\n",
    "\n",
    "    - Stratified K-Fold Cross-Validation\n",
    "        - **Definition**: A variant of K-fold cross-validation where the folds are created in such a way that each fold has approximately the same proportion of class labels as the original dataset.\n",
    "        - **Purpose**: Ensures that each fold is representative of the entire dataset, especially useful for imbalanced datasets.\n",
    "\n",
    "    - Leave-One-Out Cross-Validation (LOOCV)\n",
    "        - **Definition**: A special case of K-fold cross-validation where K equals the number of observations in the dataset. Each observation serves as a validation set exactly once.\n",
    "        - **Purpose**: Provides a thorough validation but can be computationally expensive.\n",
    "\n",
    "    - Leave-P-Out Cross-Validation\n",
    "        - **Definition**: A more general form of LOOCV where P data points are left out in each iteration. The model is trained on the remaining dataset and validated on the P data points left out.\n",
    "        - **Purpose**: Used in scenarios where specific portions of data are more critical for validation.\n",
    "\n",
    "2. Train-Test Split\n",
    "\n",
    "    - Holdout Method\n",
    "        - **Definition**: The dataset is randomly split into two parts: a training set and a testing set. The model is trained on the training set and evaluated on the testing set.\n",
    "        - **Purpose**: Simple and quick way to validate a model’s performance on unseen data.\n",
    "        - **Considerations**: The choice of the split ratio (e.g., 70/30, 80/20) can affect the evaluation results. The randomness of the split can lead to variability in the results.\n",
    "\n",
    "3. Nested Cross-Validation\n",
    "\n",
    "    - Nested Cross-Validation\n",
    "        - **Definition**: An extension of K-fold cross-validation used for hyperparameter tuning and model selection. It involves two loops: an outer loop for evaluating model performance and an inner loop for hyperparameter tuning.\n",
    "        - **Purpose**: Provides an unbiased estimate of model performance by accounting for the model selection process.\n",
    "\n",
    "4. Bootstrapping\n",
    "\n",
    "    - Bootstrapping\n",
    "        - **Definition**: Involves repeatedly sampling with replacement from the dataset and evaluating the model on each sample. The performance metrics are averaged over all samples.\n",
    "        - **Purpose**: Provides an estimate of the variability of the model’s performance and is useful for small datasets.\n",
    "\n",
    "5. Validation Techniques for Time Series Data\n",
    "\n",
    "    - Time Series Split\n",
    "        - **Definition**: The data is split based on time, keeping earlier data points for training and later data points for validation.\n",
    "        - **Purpose**: Maintains the temporal order of observations, which is crucial in time series forecasting.\n",
    "\n",
    "    - Rolling Forecast Origin\n",
    "        - **Definition**: A specific type of time series split where the training set is expanded with each iteration, and the model is validated on a fixed-size validation set.\n",
    "        - **Purpose**: Simulates a real-world scenario where new data becomes available over time.\n",
    "\n",
    "6. Performance Metrics for Model Validation\n",
    "\n",
    "    - Confusion Matrix\n",
    "        - **Definition**: A table showing the performance of a classification model by displaying the true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "    - Accuracy, Precision, Recall, F1 Score\n",
    "        - **Purpose**: Evaluate different aspects of classification model performance.\n",
    "\n",
    "    - ROC-AUC, Precision-Recall Curve\n",
    "        - **Purpose**: Evaluate the model’s performance across different thresholds.\n",
    "\n",
    "    - Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared\n",
    "        - **Purpose**: Evaluate regression model performance.\n",
    "\n",
    "    - Log-Loss\n",
    "        - **Purpose**: Measures the accuracy of probabilistic predictions in classification.\n",
    "\n",
    "These model validation techniques and strategies help ensure that the model's performance is reliable and generalizes well to new data. The choice of validation method depends on the dataset characteristics, the model's intended use, and computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583acbaf-39c6-43a7-bdb4-3328cefd561e",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n",
    "\n",
    "1. Classification Metrics\n",
    "\n",
    "    - Accuracy\n",
    "        - **Definition**: The ratio of correctly predicted instances to the total instances.\n",
    "        - **Formula**: `(TP + TN) / (TP + TN + FP + FN)`\n",
    "\n",
    "    - Precision\n",
    "        - **Definition**: The ratio of true positive predictions to the total predicted positives.\n",
    "        - **Formula**: `TP / (TP + FP)`\n",
    "\n",
    "    - Recall (Sensitivity or True Positive Rate)\n",
    "        - **Definition**: The ratio of true positive predictions to the total actual positives.\n",
    "        - **Formula**: `TP / (TP + FN)`\n",
    "\n",
    "    - F1 Score\n",
    "        - **Definition**: The harmonic mean of precision and recall.\n",
    "        - **Formula**: `2 * (Precision * Recall) / (Precision + Recall)`\n",
    "\n",
    "    - Specificity (True Negative Rate)\n",
    "        - **Definition**: The ratio of true negative predictions to the total actual negatives.\n",
    "        - **Formula**: `TN / (TN + FP)`\n",
    "\n",
    "    - AUC-ROC Curve\n",
    "        - **Definition**: AUC (Area Under the Curve) measures the ability of the classifier to distinguish between classes. ROC (Receiver Operating Characteristic) curve is a graphical plot of the true positive rate against the false positive rate.\n",
    "        - **Purpose**: Evaluates the model's performance across all classification thresholds.\n",
    "\n",
    "    - Confusion Matrix\n",
    "        - **Definition**: A table used to describe the performance of a classification model. It shows the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
    "\n",
    "2. Regression Metrics\n",
    "\n",
    "    - Mean Absolute Error (MAE)\n",
    "        - **Definition**: The average of the absolute differences between predicted and actual values.\n",
    "        - **Formula**: `(1/n) * Σ|y_i - ŷ_i|`\n",
    "\n",
    "    - Mean Squared Error (MSE)\n",
    "        - **Definition**: The average of the squared differences between predicted and actual values.\n",
    "        - **Formula**: `(1/n) * Σ(y_i - ŷ_i)²`\n",
    "\n",
    "    - Root Mean Squared Error (RMSE)\n",
    "        - **Definition**: The square root of the mean of the squared differences between predicted and actual values.\n",
    "        - **Formula**: `sqrt((1/n) * Σ(y_i - ŷ_i)²)`\n",
    "\n",
    "    - R-squared (Coefficient of Determination)\n",
    "        - **Definition**: The proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "        - **Formula**: `1 - (Σ(y_i - ŷ_i)² / Σ(y_i - ȳ)²)`\n",
    "\n",
    "    - Adjusted R-squared\n",
    "        - **Definition**: A modified version of R-squared that adjusts for the number of predictors in the model.\n",
    "        - **Purpose**: Provides a more accurate measure of model performance when multiple predictors are used.\n",
    "\n",
    "3. Clustering Metrics\n",
    "\n",
    "    - Silhouette Score\n",
    "        - **Definition**: Measures how similar an object is to its own cluster compared to other clusters.\n",
    "        - **Range**: -1 to 1, where 1 indicates that objects are well clustered.\n",
    "\n",
    "    - Davies-Bouldin Index\n",
    "        - **Definition**: Measures the average similarity ratio of each cluster with the cluster that is most similar to it.\n",
    "        - **Range**: Lower values indicate better clustering.\n",
    "\n",
    "    - Dunn Index\n",
    "        - **Definition**: Ratio of the minimum inter-cluster distance to the maximum intra-cluster distance.\n",
    "        - **Purpose**: Higher values indicate better clustering.\n",
    "\n",
    "4. Cross-Validation\n",
    "\n",
    "    - K-Fold Cross-Validation\n",
    "        - **Definition**: The dataset is divided into K equal-sized folds. The model is trained on K-1 folds and tested on the remaining fold. This process is repeated K times, and the results are averaged.\n",
    "        - **Purpose**: Provides a robust measure of model performance by minimizing overfitting.\n",
    "\n",
    "    - Leave-One-Out Cross-Validation (LOOCV)\n",
    "        - **Definition**: A special case of K-fold cross-validation where K equals the number of instances in the dataset.\n",
    "        - **Purpose**: Evaluates the model's performance by using each instance as a test set.\n",
    "\n",
    "5. Other Evaluation Techniques\n",
    "\n",
    "    - Holdout Method\n",
    "        - **Definition**: The dataset is split into training and testing sets. The model is trained on the training set and evaluated on the testing set.\n",
    "        - **Purpose**: Simple and quick evaluation method.\n",
    "\n",
    "    - Precision-Recall Curve\n",
    "        - **Definition**: A plot of the precision against recall for different threshold values.\n",
    "        - **Purpose**: Useful for evaluating models when the classes are imbalanced.\n",
    "\n",
    "    - Cost-Sensitive Metrics\n",
    "        - **Definition**: Metrics that consider the cost of false positives and false negatives, useful in cases where these errors have different consequences.\n",
    "        - **Example**: Weighted Accuracy, Cost-Adjusted ROC\n",
    "\n",
    "These evaluation techniques and metrics help in understanding the performance and reliability of different machine learning models. The choice of metric depends on the specific problem, the type of data, and the consequences of different types of errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb56df-74ac-4868-b130-d481f229f345",
   "metadata": {},
   "source": [
    "#### Save Final Model <a name='save-model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a62a32d6-2635-4f72-a524-bcecbf72d3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# final_model_filename = '../03_models/XXX_model.pkl'\n",
    "# pickle.dump(final_model, open(final_model_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724df52-9032-4120-8d93-47edf29100c5",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa82dc9-5c48-4001-89f4-5cc0171532ea",
   "metadata": {},
   "source": [
    "- **Data Dictionary**: Document the meaning and type of each column in the dataset.\n",
    "- **Exploration Report**: Summarize key findings and visualizations in a report.\n",
    "- **Next Steps**: Outline potential areas for deeper analysis or data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc72eb98-d543-470e-9a82-7c1e6a50277c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\\\n",
    "[__<< Feature Engineering__](./04_XXX_feature_engineering.ipynb) | [__Home__](../README.md)\n",
    "\n",
    "\\\n",
    "\\\n",
    "[PROJECT_NAME], _[MMMM YYYY]_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
